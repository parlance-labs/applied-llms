<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Eugene Yan">
<meta name="author" content="Bryan Bischof">
<meta name="author" content="Charles Frye">
<meta name="author" content="Hamel Husain">
<meta name="author" content="Jason Liu">
<meta name="author" content="Shreya Shankar">
<meta name="dcterms.date" content="2024-06-08">
<meta name="description" content="A practical guide to building successful LLM products.">

<title>Applied LLMs - What We‚Äôve Learned From A Year of Building with LLMs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./favicon.ico" rel="icon">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="Applied LLMs - What We‚Äôve Learned From A Year of Building with LLMs">
<meta property="og:description" content="A practical guide to building successful LLM products.">
<meta property="og:image" content="https://applied-llms.org/images/hamel_cover_img.png">
<meta property="og:site_name" content="Applied LLMs">
<meta property="og:image:height" content="540">
<meta property="og:image:width" content="960">
<meta name="twitter:title" content="Applied LLMs - What We‚Äôve Learned From A Year of Building with LLMs">
<meta name="twitter:description" content="A practical guide to building successful LLM products.">
<meta name="twitter:image" content="https://applied-llms.org/images/hamel_cover_img.png">
<meta name="twitter:image-height" content="540">
<meta name="twitter:image-width" content="960">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Applied LLMs</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./about.html"> 
<span class="menu-text">About The Authors</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#tactical-nuts-bolts-of-working-with-llms" id="toc-tactical-nuts-bolts-of-working-with-llms" class="nav-link active" data-scroll-target="#tactical-nuts-bolts-of-working-with-llms">Tactical: Nuts &amp; bolts of working with LLMs</a>
  <ul class="collapse">
  <li><a href="#prompting" id="toc-prompting" class="nav-link" data-scroll-target="#prompting">Prompting</a>
  <ul class="collapse">
  <li><a href="#focus-on-getting-the-most-out-of-fundamental-prompting-techniques" id="toc-focus-on-getting-the-most-out-of-fundamental-prompting-techniques" class="nav-link" data-scroll-target="#focus-on-getting-the-most-out-of-fundamental-prompting-techniques">Focus on getting the most out of fundamental prompting techniques</a></li>
  <li><a href="#structure-your-inputs-and-outputs" id="toc-structure-your-inputs-and-outputs" class="nav-link" data-scroll-target="#structure-your-inputs-and-outputs">Structure your inputs and outputs</a></li>
  <li><a href="#have-small-prompts-that-do-one-thing-and-only-one-thing-well" id="toc-have-small-prompts-that-do-one-thing-and-only-one-thing-well" class="nav-link" data-scroll-target="#have-small-prompts-that-do-one-thing-and-only-one-thing-well">Have small prompts that do one thing, and only one thing, well</a></li>
  <li><a href="#craft-your-context-tokens" id="toc-craft-your-context-tokens" class="nav-link" data-scroll-target="#craft-your-context-tokens">Craft your context tokens</a></li>
  </ul></li>
  <li><a href="#information-retrieval-rag" id="toc-information-retrieval-rag" class="nav-link" data-scroll-target="#information-retrieval-rag">Information Retrieval / RAG</a>
  <ul class="collapse">
  <li><a href="#the-quality-of-your-rags-output-is-dependent-on-the-quality-of-retrieved-documents-which-in-turn-can-be-considered-along-a-few-factors" id="toc-the-quality-of-your-rags-output-is-dependent-on-the-quality-of-retrieved-documents-which-in-turn-can-be-considered-along-a-few-factors" class="nav-link" data-scroll-target="#the-quality-of-your-rags-output-is-dependent-on-the-quality-of-retrieved-documents-which-in-turn-can-be-considered-along-a-few-factors">The quality of your RAG‚Äôs output is dependent on the quality of retrieved documents, which in turn can be considered along a few factors</a></li>
  <li><a href="#dont-forget-keyword-search-use-it-as-a-baseline-and-in-hybrid-search" id="toc-dont-forget-keyword-search-use-it-as-a-baseline-and-in-hybrid-search" class="nav-link" data-scroll-target="#dont-forget-keyword-search-use-it-as-a-baseline-and-in-hybrid-search">Don‚Äôt forget keyword search; use it as a baseline and in hybrid search</a></li>
  <li><a href="#prefer-rag-over-fine-tuning-for-new-knowledge" id="toc-prefer-rag-over-fine-tuning-for-new-knowledge" class="nav-link" data-scroll-target="#prefer-rag-over-fine-tuning-for-new-knowledge">Prefer RAG over fine-tuning for new knowledge</a></li>
  <li><a href="#long-context-models-wont-make-rag-obsolete" id="toc-long-context-models-wont-make-rag-obsolete" class="nav-link" data-scroll-target="#long-context-models-wont-make-rag-obsolete">Long-context models won‚Äôt make RAG obsolete</a></li>
  </ul></li>
  <li><a href="#tuning-and-optimizing-workflows" id="toc-tuning-and-optimizing-workflows" class="nav-link" data-scroll-target="#tuning-and-optimizing-workflows">Tuning and optimizing workflows</a>
  <ul class="collapse">
  <li><a href="#step-by-step-multi-turn-flows-can-give-large-boosts" id="toc-step-by-step-multi-turn-flows-can-give-large-boosts" class="nav-link" data-scroll-target="#step-by-step-multi-turn-flows-can-give-large-boosts">Step-by-step, multi-turn ‚Äúflows‚Äù can give large boosts</a></li>
  <li><a href="#prioritize-deterministic-workflows-for-now" id="toc-prioritize-deterministic-workflows-for-now" class="nav-link" data-scroll-target="#prioritize-deterministic-workflows-for-now">Prioritize deterministic workflows for now</a></li>
  <li><a href="#getting-more-diverse-outputs-beyond-temperature" id="toc-getting-more-diverse-outputs-beyond-temperature" class="nav-link" data-scroll-target="#getting-more-diverse-outputs-beyond-temperature">Getting more diverse outputs beyond temperature</a></li>
  <li><a href="#caching-is-underrated" id="toc-caching-is-underrated" class="nav-link" data-scroll-target="#caching-is-underrated">Caching is underrated</a></li>
  <li><a href="#when-to-finetune" id="toc-when-to-finetune" class="nav-link" data-scroll-target="#when-to-finetune">When to finetune</a></li>
  </ul></li>
  <li><a href="#evaluation-monitoring" id="toc-evaluation-monitoring" class="nav-link" data-scroll-target="#evaluation-monitoring">Evaluation &amp; Monitoring</a>
  <ul class="collapse">
  <li><a href="#create-a-few-assertion-based-unit-tests-from-real-inputoutput-samples" id="toc-create-a-few-assertion-based-unit-tests-from-real-inputoutput-samples" class="nav-link" data-scroll-target="#create-a-few-assertion-based-unit-tests-from-real-inputoutput-samples">Create a few assertion-based unit tests from real input/output samples</a></li>
  <li><a href="#llm-as-judge-can-work-somewhat-but-its-not-a-silver-bullet" id="toc-llm-as-judge-can-work-somewhat-but-its-not-a-silver-bullet" class="nav-link" data-scroll-target="#llm-as-judge-can-work-somewhat-but-its-not-a-silver-bullet">LLM-as-Judge can work (somewhat), but it‚Äôs not a silver bullet</a></li>
  <li><a href="#the-intern-test-for-evaluating-generations" id="toc-the-intern-test-for-evaluating-generations" class="nav-link" data-scroll-target="#the-intern-test-for-evaluating-generations">The ‚Äúintern test‚Äù for evaluating generations</a></li>
  <li><a href="#overemphasizing-certain-evals-can-hurt-overall-performance" id="toc-overemphasizing-certain-evals-can-hurt-overall-performance" class="nav-link" data-scroll-target="#overemphasizing-certain-evals-can-hurt-overall-performance">Overemphasizing certain evals can hurt overall performance</a></li>
  <li><a href="#simplify-annotation-to-binary-tasks-or-pairwise-comparisons" id="toc-simplify-annotation-to-binary-tasks-or-pairwise-comparisons" class="nav-link" data-scroll-target="#simplify-annotation-to-binary-tasks-or-pairwise-comparisons">Simplify annotation to binary tasks or pairwise comparisons</a></li>
  <li><a href="#reference-free-evals-and-guardrails-can-be-used-interchangeably" id="toc-reference-free-evals-and-guardrails-can-be-used-interchangeably" class="nav-link" data-scroll-target="#reference-free-evals-and-guardrails-can-be-used-interchangeably">(Reference-free) evals and guardrails can be used interchangeably</a></li>
  <li><a href="#llms-will-return-output-even-when-they-shouldnt" id="toc-llms-will-return-output-even-when-they-shouldnt" class="nav-link" data-scroll-target="#llms-will-return-output-even-when-they-shouldnt">LLMs will return output even when they shouldn‚Äôt</a></li>
  <li><a href="#hallucinations-are-a-stubborn-problem" id="toc-hallucinations-are-a-stubborn-problem" class="nav-link" data-scroll-target="#hallucinations-are-a-stubborn-problem">Hallucinations are a stubborn problem</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#operational-day-to-day-and-org-concerns" id="toc-operational-day-to-day-and-org-concerns" class="nav-link" data-scroll-target="#operational-day-to-day-and-org-concerns">Operational: Day-to-day and org concerns</a>
  <ul class="collapse">
  <li><a href="#data" id="toc-data" class="nav-link" data-scroll-target="#data">Data</a>
  <ul class="collapse">
  <li><a href="#check-for-development-prod-skew" id="toc-check-for-development-prod-skew" class="nav-link" data-scroll-target="#check-for-development-prod-skew">Check for development-prod skew</a></li>
  <li><a href="#look-at-samples-of-llm-inputs-and-outputs-every-day" id="toc-look-at-samples-of-llm-inputs-and-outputs-every-day" class="nav-link" data-scroll-target="#look-at-samples-of-llm-inputs-and-outputs-every-day">Look at samples of LLM inputs and outputs every day</a></li>
  </ul></li>
  <li><a href="#working-with-models" id="toc-working-with-models" class="nav-link" data-scroll-target="#working-with-models">Working with models</a>
  <ul class="collapse">
  <li><a href="#generate-structured-output-to-ease-downstream-integration" id="toc-generate-structured-output-to-ease-downstream-integration" class="nav-link" data-scroll-target="#generate-structured-output-to-ease-downstream-integration">Generate structured output to ease downstream integration</a></li>
  <li><a href="#migrating-prompts-across-models-is-a-pain-in-the-ass" id="toc-migrating-prompts-across-models-is-a-pain-in-the-ass" class="nav-link" data-scroll-target="#migrating-prompts-across-models-is-a-pain-in-the-ass">Migrating prompts across models is a pain in the ass</a></li>
  <li><a href="#version-and-pin-your-models" id="toc-version-and-pin-your-models" class="nav-link" data-scroll-target="#version-and-pin-your-models">Version and pin your models</a></li>
  <li><a href="#choose-the-smallest-model-that-gets-the-job-done" id="toc-choose-the-smallest-model-that-gets-the-job-done" class="nav-link" data-scroll-target="#choose-the-smallest-model-that-gets-the-job-done">Choose the smallest model that gets the job done</a></li>
  </ul></li>
  <li><a href="#product" id="toc-product" class="nav-link" data-scroll-target="#product">Product</a>
  <ul class="collapse">
  <li><a href="#involve-design-early-and-often" id="toc-involve-design-early-and-often" class="nav-link" data-scroll-target="#involve-design-early-and-often">Involve design early and often</a></li>
  <li><a href="#design-your-ux-for-human-in-the-loop" id="toc-design-your-ux-for-human-in-the-loop" class="nav-link" data-scroll-target="#design-your-ux-for-human-in-the-loop">Design your UX for Human-In-The-Loop</a></li>
  <li><a href="#prioritize-your-hierarchy-of-needs-ruthlessly" id="toc-prioritize-your-hierarchy-of-needs-ruthlessly" class="nav-link" data-scroll-target="#prioritize-your-hierarchy-of-needs-ruthlessly">Prioritize your hierarchy of needs ruthlessly</a></li>
  <li><a href="#calibrate-your-risk-tolerance-based-on-the-use-case" id="toc-calibrate-your-risk-tolerance-based-on-the-use-case" class="nav-link" data-scroll-target="#calibrate-your-risk-tolerance-based-on-the-use-case">Calibrate your risk tolerance based on the use case</a></li>
  </ul></li>
  <li><a href="#team-roles" id="toc-team-roles" class="nav-link" data-scroll-target="#team-roles">Team &amp; Roles</a>
  <ul class="collapse">
  <li><a href="#focus-on-process-not-tools" id="toc-focus-on-process-not-tools" class="nav-link" data-scroll-target="#focus-on-process-not-tools">Focus on process, not tools</a></li>
  <li><a href="#always-be-experimenting" id="toc-always-be-experimenting" class="nav-link" data-scroll-target="#always-be-experimenting">Always be experimenting</a></li>
  <li><a href="#empower-everyone-to-use-new-ai-technology" id="toc-empower-everyone-to-use-new-ai-technology" class="nav-link" data-scroll-target="#empower-everyone-to-use-new-ai-technology">Empower everyone to use new AI technology</a></li>
  <li><a href="#dont-fall-into-the-trap-of-ai-engineering-is-all-i-need" id="toc-dont-fall-into-the-trap-of-ai-engineering-is-all-i-need" class="nav-link" data-scroll-target="#dont-fall-into-the-trap-of-ai-engineering-is-all-i-need">Don‚Äôt fall into the trap of ‚ÄúAI Engineering is all I need‚Äù</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#strategic-long-term-business-strategy-pending" id="toc-strategic-long-term-business-strategy-pending" class="nav-link" data-scroll-target="#strategic-long-term-business-strategy-pending">Strategic: Long-term business strategy (pending)</a>
  <ul class="collapse">
  <li><a href="#stay-in-touch" id="toc-stay-in-touch" class="nav-link" data-scroll-target="#stay-in-touch">Stay In Touch</a></li>
  <li><a href="#acknowledgements" id="toc-acknowledgements" class="nav-link" data-scroll-target="#acknowledgements">Acknowledgements</a></li>
  <li><a href="#about-the-authors" id="toc-about-the-authors" class="nav-link" data-scroll-target="#about-the-authors">About the authors</a></li>
  </ul></li>
  </ul>
</nav>
    <div class="quarto-margin-footer"><div class="margin-footer-item">
<script async="" data-uid="b3e2fda9e7" src="https://appliedllms.ck.page/b3e2fda9e7/index.js"></script>
</div></div></div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">What We‚Äôve Learned From A Year of Building with LLMs</h1>
</div>

<div>
  <div class="description">
    A practical guide to building successful LLM products.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-contents">
             <p><a href="https://eugeneyan.com">Eugene Yan</a> </p>
             <p><a href="https://www.linkedin.com/in/bryan-bischof/">Bryan Bischof</a> </p>
             <p><a href="https://www.linkedin.com/in/charles-frye-38654abb/">Charles Frye</a> </p>
             <p><a href="https://hamel.dev">Hamel Husain</a> </p>
             <p><a href="https://jxnl.co/">Jason Liu</a> </p>
             <p><a href="https://www.sh-reya.com/">Shreya Shankar</a> </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 8, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>It‚Äôs an exciting time to build with large language models (LLMs). Over the past year, LLMs have become ‚Äúgood enough‚Äù for real-world applications. And they‚Äôre getting better and cheaper every year. Coupled with a parade of demos on social media, there will be an <a href="https://www.goldmansachs.com/intelligence/pages/ai-investment-forecast-to-approach-200-billion-globally-by-2025.html" target="&quot;_blank">estimated $200B investment in AI by 2025</a>. Furthermore, provider APIs have made LLMs more accessible, allowing everyone, not just ML engineers and scientists, to build intelligence into their products. Nonetheless, while the barrier to entry for building with AI has been lowered, creating products and systems that are effective‚Äîbeyond a demo‚Äîremains deceptively difficult.</p>
<p>We‚Äôve spent the past year building, and have discovered many sharp edges along the way. While we don‚Äôt claim to speak for the entire industry, we‚Äôd like to share what we‚Äôve learned to help you avoid our mistakes and iterate faster. These are organized into three sections:</p>
<ul>
<li><a href="#tactical-nuts--bolts-of-working-with-llms">Tactical</a>: Some practices for prompting, RAG, flow engineering, evals, and monitoring. Whether you‚Äôre a practitioner building with LLMs, or hacking on weekend projects, this section was written for you.</li>
<li><a href="#operation-day-to-day-and-org-concerns">Operational</a>: The organizational, day-to-day concerns of shipping products, and how to build an effective team. For product/technical leaders looking to deploy sustainably and reliably.</li>
<li>Strategic: The long-term, big-picture view, with opinionated takes such as ‚Äúno GPU before PMF‚Äù and ‚Äúfocus on the system not the model‚Äù, and how to iterate. Written with founders and executives in mind.</li>
</ul>
<p>Our intent is to make this a practical guide to building successful products with LLMs, drawing from our own experiences and pointing to examples from around the industry.</p>
<p>Ready to <del>delve</del> dive in? Let‚Äôs go.</p>
<hr>
<section id="tactical-nuts-bolts-of-working-with-llms" class="level1">
<h1>Tactical: Nuts &amp; bolts of working with LLMs</h1>
<p>In this section, we share some best practices for the core components of the emerging LLM stack: prompting tips to improve quality and reliability, evaluation strategies to assess output, retrieval-augmented generation ideas to improve grounding, and more. We‚Äôll also explore how to design human-in-the-loop workflows. While the technology is still rapidly developing, we hope that these lessons, the by-product of countless experiments we‚Äôve collectively run, will stand the test of time and help you build and ship robust LLM applications.</p>
<section id="prompting" class="level2">
<h2 class="anchored" data-anchor-id="prompting">Prompting</h2>
<p>We recommend starting with prompting when developing new applications. It‚Äôs easy to both underestimate and overestimate its importance. It‚Äôs underestimated because the right prompting techniques, when used correctly, can get us very far. It‚Äôs overestimated because even prompt-based applications require significant engineering around the prompt to work well.</p>
<section id="focus-on-getting-the-most-out-of-fundamental-prompting-techniques" class="level3">
<h3 class="anchored" data-anchor-id="focus-on-getting-the-most-out-of-fundamental-prompting-techniques">Focus on getting the most out of fundamental prompting techniques</h3>
<p>A few prompting techniques have consistently helped with improving performance across a variety of models and tasks: n-shot prompts + in-context learning, chain-of-thought, and providing relevant resources.</p>
<p>The idea of in-context learning via n-shot prompts is to provide the LLM with a few examples that demonstrate the task and align outputs to our expectations. A few tips: - If n is too low, the model may over-anchor on those specific examples, hurting its ability to generalize. As a rule of thumb, aim for n ‚â• 5. Don‚Äôt be afraid to go as high as a few dozen. - Examples should be representative of the expected input distribution. If you‚Äôre building a movie summarizer, include samples from different genres in roughly the same proportion you‚Äôd expect to see in practice. - You don‚Äôt necessarily need to provide the full input-output pairs. In many cases, examples of desired outputs are sufficient. - If using an LLM which supports tool use, your n-shot examples should also use the tools you want the agent to use.</p>
<p>In Chain-of-Thought (CoT) prompting, we encourage the LLM to explain its thought process before returning the final answer. Think of it as providing the LLM with a sketchpad so it doesn‚Äôt have to do it all in memory. The original approach was to simply add the phrase ‚ÄúLet‚Äôs think step-by-step‚Äù as part of the instructions, but, we‚Äôve found it helpful to make the CoT more specific, where adding specificity via an extra sentence or two often reduces hallucination rates significantly. For example, when asking an LLM to summarize a meeting transcript, we can be explicit about the steps, such as: - First, list out the key decisions, follow-up items, and associated owners in a sketchpad. - Then, check that the details in the sketchpad are factually consistent with the transcript. - Finally, synthesize the key points into a concise summary.</p>
<p>Note that in recent times, <a href="https://arxiv.org/abs/2405.04776">some doubt</a> has been cast on if this technique is as powerful as believed. Additionally, there‚Äôs significant debate as to exactly what is going on during inference when Chain-of-Thought is being used. Regardless, this technique is one to experiment with when possible.</p>
<p>Providing relevant resources is a powerful mechanism to expand the model‚Äôs knowledge base, reduce hallucinations, and increase the user‚Äôs trust. Often accomplished via Retrieval Augmented Generation (RAG), providing the model with snippets of text that it can directly utilize in its response is an essential technique. When providing the relevant resources, it‚Äôs not enough to merely include them; don‚Äôt forget to tell the model to prioritize their use, refer to them directly, and sometimes to mention when none of the resources are sufficient. These help ‚Äúground‚Äù agent responses to a corpus of resources.</p>
</section>
<section id="structure-your-inputs-and-outputs" class="level3">
<h3 class="anchored" data-anchor-id="structure-your-inputs-and-outputs">Structure your inputs and outputs</h3>
<p>Structured input and output help models better understand the input as well as return output that can reliably integrate with downstream systems. Adding serialization formatting to your inputs can help provide more clues to the model as to the relationships between tokens in the context, additional metadata to specific tokens (like types), or relate the request to similar examples in the model‚Äôs training data.</p>
<p>As an example, many questions on the internet about writing SQL begin by specifying the SQL schema. Thus, you may expect that effective prompting for Text-to-SQL should include structured schema definitions; <a href="https://www.researchgate.net/publication/371223615_SQL-PaLM_Improved_Large_Language_ModelAdaptation_for_Text-to-SQL">indeed</a></p>
<p>Structured output serves a similar purpose, but it also simplifies integration into downstream components of your system. <a href="https://github.com/jxnl/instructor">Instructor</a> and <a href="https://github.com/outlines-dev/outlines">Outlines</a> work well for structured output. (If you‚Äôre importing an LLM API SDK, use Instructor; if you‚Äôre importing Huggingface for a self-hosted model, use Outlines.) Structured input expresses tasks clearly and resembles how the training data is formatted, increasing the probability of better output.</p>
<p>When using structured input, be aware that each LLM family has their own preferences. Claude prefers <code>&lt;xml&gt;</code> while GPT favors Markdown and JSON. With XML, you can even pre-fill Claude‚Äôs responses by providing a <code>&lt;response&gt;</code> tag like so.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>messages<span class="op">=</span>[</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">"content"</span>: <span class="st">"""Extract the &lt;name&gt;, &lt;size&gt;, &lt;price&gt;, and &lt;color&gt; from this product description into your &lt;response&gt;.</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="st">            &lt;description&gt;The SmartHome Mini is a compact smart home assistant available in black or white for only $49.99. At just 5 inches wide, it lets you control lights, thermostats, and other connected devices via voice or app‚Äîno matter where you place it in your home. This affordable little hub brings convenient hands-free control to your smart devices.</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="st">            &lt;/description&gt;"""</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">"role"</span>: <span class="st">"assistant"</span>,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">"content"</span>: <span class="st">"&lt;response&gt;&lt;name&gt;"</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="have-small-prompts-that-do-one-thing-and-only-one-thing-well" class="level3">
<h3 class="anchored" data-anchor-id="have-small-prompts-that-do-one-thing-and-only-one-thing-well">Have small prompts that do one thing, and only one thing, well</h3>
<p>A common anti-pattern / code smell in software is the ‚Äú<a href="https://en.wikipedia.org/wiki/God_object">God Object</a>‚Äù, where we have a single class or function that does everything. The same applies to prompts too.</p>
<p>A prompt typically starts simple: A few sentences of instruction, a couple of examples, and we‚Äôre good to go. But as we try to improve performance and handle more edge cases, complexity creeps in. More instructions. Multi-step reasoning. Dozens of examples. Before we know it, our initially simple prompt is now a 2,000 token frankenstein. And to add injury to insult, it has worse performance on the more common and straightforward inputs! GoDaddy shared this challenge as their <a href="https://www.godaddy.com/resources/news/llm-from-the-trenches-10-lessons-learned-operationalizing-models-at-godaddy#h-1-sometimes-one-prompt-isn-t-enough">No.&nbsp;1 lesson from building with LLMs</a>.</p>
<p>Just like how we strive (read: struggle) to keep our systems and code simple, so should we for our prompts. Instead of having a single, catch-all prompt for the meeting transcript summarizer, we can break it into steps to: - Extract key decisions, action items, and owners into structured format - Check extracted details against the original transcription for consistency - Generate a concise summary from the structured details</p>
<p>As a result, we‚Äôve split our single prompt into multiple prompts that are each simple, focused, and easy to understand. And by breaking them up, we can now iterate and eval each prompt individually.</p>
</section>
<section id="craft-your-context-tokens" class="level3">
<h3 class="anchored" data-anchor-id="craft-your-context-tokens">Craft your context tokens</h3>
<p>Rethink, and challenge your assumptions about how much context you actually need to send to the agent. Be like Michaelangelo, do not build up your context sculpture ‚Äì chisel away the superfluous material until the sculpture is revealed. RAG is a popular way to collate all of the potentially relevant blocks of marble, but what are you doing to extract what‚Äôs necessary?</p>
<p>We‚Äôve found that taking the final prompt sent to the model ‚Äì with all of the context construction, and meta-prompting, and RAG results ‚Äì putting it on a blank page and just reading it, really helps you rethink your context. We have found redundancy, self-contradictory language, and poor formatting using this method.</p>
<p>The other key optimization is the structure of your context. Your bag-of-docs representation isn‚Äôt helpful for humans, don‚Äôt assume it‚Äôs any good for agents. Think carefully about how you structure your context to underscore the relationships between parts of it, and make extraction as simple as possible.</p>
</section>
</section>
<section id="information-retrieval-rag" class="level2">
<h2 class="anchored" data-anchor-id="information-retrieval-rag">Information Retrieval / RAG</h2>
<p>Beyond prompting, another effective way to steer an LLM is by providing knowledge as part of the prompt. This grounds the LLM on the provided context which is then used for in-context learning. This is known as retrieval-augmented generation (RAG). Practitioners have found RAG effective at providing knowledge and improving output, while requiring far less effort and cost compared to finetuning. RAG is only as good as the retrieved documents‚Äô relevance, density, and detail</p>
<section id="the-quality-of-your-rags-output-is-dependent-on-the-quality-of-retrieved-documents-which-in-turn-can-be-considered-along-a-few-factors" class="level3">
<h3 class="anchored" data-anchor-id="the-quality-of-your-rags-output-is-dependent-on-the-quality-of-retrieved-documents-which-in-turn-can-be-considered-along-a-few-factors">The quality of your RAG‚Äôs output is dependent on the quality of retrieved documents, which in turn can be considered along a few factors</h3>
<p>The first and most obvious metric is relevance. This is typically quantified via ranking metrics such as <a href="https://en.wikipedia.org/wiki/Mean_reciprocal_rank">Mean Reciprocal Rank (MRR)</a> or <a href="https://en.wikipedia.org/wiki/Discounted_cumulative_gain">Normalized Discounted Cumulative Gain (NDCG)</a>. MRR evaluates how well a system places the first relevant result in a ranked list while NDCG considers the relevance of all the results and their positions. They measure how good the system is at ranking relevant documents higher and irrelevant documents lower. For example, if we‚Äôre retrieving user summaries to generate movie review summaries, we‚Äôll want to rank reviews for the specific movie higher while excluding reviews for other movies.</p>
<p>Like traditional recommendation systems, the rank of retrieved items will have a significant impact on how the LLM performs on downstream tasks. To measure the impact, run a RAG-based task but with the retrieved items shuffled‚Äîhow does the RAG output perform?</p>
<p>Second, we also want to consider information density. If two documents are equally relevant, we should prefer one that‚Äôs more concise and has lesser extraneous details. Returning to our movie example, we might consider the movie transcript and all user reviews to be relevant in a broad sense. Nonetheless, the top-rated reviews and editorial reviews will likely be more dense in information.</p>
<p>Finally, consider the level of detail provided in the document. Imagine we‚Äôre building a RAG system to generate SQL queries from natural language. We could simply provide table schemas with column names as context. But, what if we include column descriptions and some representative values? The additional detail could help the LLM better understand the semantics of the table and thus generate more correct SQL.</p>
</section>
<section id="dont-forget-keyword-search-use-it-as-a-baseline-and-in-hybrid-search" class="level3">
<h3 class="anchored" data-anchor-id="dont-forget-keyword-search-use-it-as-a-baseline-and-in-hybrid-search">Don‚Äôt forget keyword search; use it as a baseline and in hybrid search</h3>
<p>Given how prevalent the embedding-based RAG demo is, it‚Äôs easy to forget or overlook the decades of research and solutions in information retrieval.</p>
<p>Nonetheless, while embeddings are undoubtedly a powerful tool, they are not the be all and end all. First, while they excel at capturing high-level semantic similarity, they may struggle with more specific, keyword-based queries, like when users search for names (e.g., Ilya), acronyms (e.g., RAG), or IDs (e.g., claude-3-sonnet). Keyword-based search, such as BM25, are explicitly designed for this. And after years of keyword-based search, users have likely taken it for granted and may get frustrated if the document they expect to retrieve isn‚Äôt being returned.</p>
<blockquote class="blockquote">
<p>Vector embeddings <em>do not</em> magically solve search. In fact, the heavy lifting is in the step before you re-rank with semantic similarity search. Making a genuine improvement over BM25 or full-text search is hard. ‚Äî <a href="https://x.com/AravSrinivas/status/1737886080555446552">Aravind Srinivas, CEO Perplexity.ai</a></p>
</blockquote>
<blockquote class="blockquote">
<p>We‚Äôve been communicating this to our customers and partners for months now. Nearest Neighbor Search with naive embeddings yields very noisy results and you‚Äôre likely better off starting with a keyword-based approach. ‚Äî <a href="https://twitter.com/beyang/status/1767330006999720318">Beyang Liu, CTO Sourcegraph</a></p>
</blockquote>
<p>Second, it‚Äôs more straightforward to understand why a document was retrieved with keyword search‚Äîwe can look at the keywords that match the query. In contrast, embedding-based retrieval is less interpretable. Finally, thanks to systems like Lucene and OpenSearch that have been optimized and battle-tested over decades, keyword search is usually more computationally efficient.</p>
<p>In most cases, a hybrid will work best: keyword matching for the obvious matches, and embeddings for synonyms, hypernyms, and spelling errors, as well as multimodality (e.g., images and text). <a href="https://www.shortwave.com/blog/deep-dive-into-worlds-smartest-email-ai/">Shortwave shared how they built their RAG pipeline</a>, including query rewriting, keyword + embedding retrieval, and ranking.</p>
</section>
<section id="prefer-rag-over-fine-tuning-for-new-knowledge" class="level3">
<h3 class="anchored" data-anchor-id="prefer-rag-over-fine-tuning-for-new-knowledge">Prefer RAG over fine-tuning for new knowledge</h3>
<p>Both RAG and fine-tuning can be used to incorporate new information into LLMs and increase performance on specific tasks. Thus, which should we try first?</p>
<p>Recent research suggests that RAG may have an edge. <a href="https://arxiv.org/abs/2312.05934">One study</a> compared RAG against unsupervised finetuning (aka continued pretraining), evaluating both on a subset of MMLU and current events. They found that RAG consistently outperformed fine-tuning for knowledge encountered during training as well as entirely new knowledge. In <a href="https://arxiv.org/abs/2401.08406">another paper</a>, they compared RAG against supervised finetuning on an agricultural dataset. Similarly, the performance boost from RAG was greater than fine-tuning, especially for GPT-4 (see Table 20 of the paper).</p>
<p>Beyond improved performance, RAG comes with several practical advantages too. First, compared to continuous pretraining or fine-tuning, it‚Äôs easier‚Äîand cheaper!‚Äîto keep retrieval indices up-to-date. Second, if our retrieval indices have problematic documents that contain toxic or biased content, we can easily drop or modify the offending documents.</p>
<p>In addition, the R in RAG provides finer grained control over how we retrieve documents. For example, if we‚Äôre hosting a RAG system for multiple organizations, by partitioning the retrieval indices, we can ensure that each organization can only retrieve documents from their own index. This ensures that we don‚Äôt inadvertently expose information from one organization to another.</p>
</section>
<section id="long-context-models-wont-make-rag-obsolete" class="level3">
<h3 class="anchored" data-anchor-id="long-context-models-wont-make-rag-obsolete">Long-context models won‚Äôt make RAG obsolete</h3>
<p>With Gemini 1.5 providing context windows of up to 10M tokens in size, some have begun to question the future of RAG.</p>
<blockquote class="blockquote">
<p>I tend to believe that Gemini 1.5 is significantly overhyped by Sora. A context window of 10M tokens effectively makes most of existing RAG frameworks unnecessary ‚Äî you simply put whatever your data into the context and talk to the model like usual. Imagine how it does to all the startups / agents / langchain projects where most of the engineering efforts goes to RAG üòÖ Or in one sentence: the 10m context kills RAG. Nice work Gemini ‚Äî <a href="https://x.com/Francis_YAO_/status/1758935954189115714">Yao Fu</a></p>
</blockquote>
<p>While it‚Äôs true that long contexts will be a game-changer for use cases such as analyzing multiple documents or chatting with PDFs, the rumors of RAG‚Äôs demise are greatly exaggerated.</p>
<p>First, even with a context window of 10M tokens, we‚Äôd still need a way to select information to feed into the model. Second, beyond the narrow needle-in-a-haystack eval, we‚Äôve yet to see convincing data that models can effectively reason over such a large context. Thus, without good retrieval (and ranking), we risk overwhelming the model with distractors, or may even fill the context window with completely irrelevant information.</p>
<p>Finally, there‚Äôs cost. The Transformer‚Äôs inference cost scales quadratically (or linearly in both space and time) with context length. Just because there exists a model that could read your organization‚Äôs entire Google Drive contents before answering each question doesn‚Äôt mean that‚Äôs a good idea. Consider an analogy to how we use RAM: we still read and write from disk, even though there exist compute instances with <a href="https://aws.amazon.com/ec2/instance-types/high-memory/">RAM running into the tens of terabytes</a>.</p>
<p>So don‚Äôt throw your RAGs in the trash just yet. This pattern will remain useful even as context windows grow in size.</p>
</section>
</section>
<section id="tuning-and-optimizing-workflows" class="level2">
<h2 class="anchored" data-anchor-id="tuning-and-optimizing-workflows">Tuning and optimizing workflows</h2>
<p>Prompting an LLM is just the beginning. To get the most juice out of them, we need to think beyond a single prompt and embrace workflows. For example, how could we split a single complex task into multiple simpler tasks? When is finetuning or caching helpful with increasing performance and reducing latency/cost? In this section, we share proven strategies and real-world examples to help you optimize and build reliable LLM workflows.</p>
<section id="step-by-step-multi-turn-flows-can-give-large-boosts" class="level3">
<h3 class="anchored" data-anchor-id="step-by-step-multi-turn-flows-can-give-large-boosts">Step-by-step, multi-turn ‚Äúflows‚Äù can give large boosts</h3>
<p>We already know that by decomposing a single big prompt into multiple smaller prompts, we can achieve better results. An example of this is <a href="https://arxiv.org/abs/2401.08500">AlphaCodium</a>: By switching from a single prompt to a multi-step workflow, they increased GPT-4 accuracy (pass@5) on CodeContests from 19% to 44%. The workflow includes: - Reflecting on the problem - Reasoning on the public tests - Generating possible solutions - Ranking possible solutions - Generating synthetic tests - Iterating on the solutions on public and synthetic tests.</p>
<p>Small tasks with clear objectives make for the best agent or flow prompts. It‚Äôs not required that every agent prompt requests structured output, but structured outputs help a lot to interface with whatever system is orchestrating the agent‚Äôs interactions with the environment.</p>
<p>Some things to try: - An explicit planning step, as tightly specified as possible. Consider having <a href="https://youtu.be/hGXhFa3gzBs?si=gNEGYzux6TuB1del">predefined plans to choose from</a>. - Rewriting the original user prompts into agent prompts. Be careful, this process is lossy! - Agent behaviors as linear chains, DAGs, and State-Machines; different dependency and logic relationships can be more and less appropriate for different scales. Can you squeeze performance optimization out of different task architectures? - Planning validations; your planning can include instructions on how to evaluate the responses from other agents to make sure the final assembly works well together. - Prompt engineering with fixed upstream state‚Äîmake sure your agent prompts are evaluated against a collection of variants of what may happen before.</p>
</section>
<section id="prioritize-deterministic-workflows-for-now" class="level3">
<h3 class="anchored" data-anchor-id="prioritize-deterministic-workflows-for-now">Prioritize deterministic workflows for now</h3>
<p>While AI agents can dynamically react to user requests and the environment, their non-deterministic nature makes them a challenge to deploy. Each step an agent takes has a chance of failing, and the chances of recovering from the error are poor. Thus, the likelihood that an agent completes a multi-step task successfully decreases exponentially as the number of steps increases. As a result, teams building agents find it difficult to deploy reliable agents.</p>
<p>A promising approach is to have agent systems that produce deterministic plans which are then executed in a structured, reproducible way. In the first step, given a high-level goal or prompt, the agent generates a plan. Then, the plan is executed deterministically. This allows each step to be more predictable and reliable. Benefits include: - Generated plans can serve as few-shot samples to prompt or finetune an agent. - Deterministic execution makes the system more reliable, and thus easier to test and debug. Furthermore, failures can be traced to the specific steps in the plan. - Generated plans can be represented as directed acyclic graphs (DAGs) which are easier, relative to a static prompt, to understand and adapt to new situations.</p>
<p>The most successful agent builders may be those with strong experience managing junior engineers because the process of generating plans is similar to how we instruct and manage juniors. We give juniors clear goals and concrete plans, instead of vague open-ended directions, and we should do the same for our agents too.</p>
<p>In the end, the key to reliable, working agents will likely be found in adopting more structured, deterministic approaches, as well as collecting data to refine prompts and finetune models. Without this, we‚Äôll build agents that may work exceptionally well some of the time, but on average, disappoint users which leads to poor retention.</p>
</section>
<section id="getting-more-diverse-outputs-beyond-temperature" class="level3">
<h3 class="anchored" data-anchor-id="getting-more-diverse-outputs-beyond-temperature">Getting more diverse outputs beyond temperature</h3>
<p>Suppose your task requires diversity in an LLM‚Äôs output. Maybe you‚Äôre writing an LLM pipeline to suggest products to buy from your catalog given a list of products the user bought previously. When running your prompt multiple times, you might notice that the resulting recommendations are too similar‚Äîso you might increase the temperature parameter in your LLM requests.</p>
<p>Briefly, increasing the temperature parameter makes LLM responses more varied. At sampling time, the probability distributions of the next token become flatter, meaning that tokens which are usually less likely get chosen more often. Still, when increasing temperature, you may notice some failure modes related to output diversity. For example, Some products from the catalog that could be a good fit may never be output by the LLM. The same handful of products might be overrepresented in outputs, if they are highly likely to follow the prompt based on what the LLM has learned at training time. If the temperature is too high, you may get outputs that reference nonexistent products (or gibberish!)</p>
<p>In other words, increasing temperature does not guarantee that the LLM will sample outputs from the probability distribution you expect (e.g., uniform random). Nonetheless, we have other tricks to increase output diversity. The simplest way is to adjust elements within the prompt. For example, if the prompt template includes a list of items, such as historical purchases, shuffling the order of these items each time they‚Äôre inserted into the prompt can make a significant difference.</p>
<p>Additionally, keeping a short list of recent outputs can help prevent redundancy. In our recommended products example, by instructing the LLM to avoid suggesting items from this recent list, or by rejecting and resampling outputs that are similar to recent suggestions, we can further diversify the responses. Another effective strategy is to vary the phrasing used in the prompts. For instance, incorporating phrases like ‚Äúpick an item that the user would love using regularly‚Äù or ‚Äúselect a product that the user would likely recommend to friends‚Äù can shift the focus and thereby influence the variety of recommended products.</p>
</section>
<section id="caching-is-underrated" class="level3">
<h3 class="anchored" data-anchor-id="caching-is-underrated">Caching is underrated</h3>
<p>Caching saves cost and eliminates generation latency by removing the need to recompute responses for the same input. Furthermore, if a response has previously been guardrailed, we can serve these vetted responses and reduce the risk of serving harmful or inappropriate content.</p>
<p>One straightforward approach to caching is to use unique IDs for the items being processed, such as if we‚Äôre summarizing new articles or <a href="https://www.cnbc.com/2023/06/12/amazon-is-using-generative-ai-to-summarize-product-reviews.html">product reviews</a>. When a request comes in, we can check to see if a summary already exists in the cache. If so, we can return it immediately; if not, we generate, guardrail, and serve it, and then store it in the cache for future requests.</p>
<p>For more open-ended queries, we can borrow techniques from the field of search, which also leverages caching for open-ended inputs. Features like autocomplete and spelling correction also help normalize user input and thus increase the cache hit rate.</p>
</section>
<section id="when-to-finetune" class="level3">
<h3 class="anchored" data-anchor-id="when-to-finetune">When to finetune</h3>
<p>We may have some tasks where even the most cleverly designed prompts fall short. For example, even after significant prompt engineering, our system may still be a ways from returning reliable, high-quality output. If so, then it may be necessary to finetune a model for your specific task.</p>
<p>Successful examples include: - <a href="https://www.honeycomb.io/blog/introducing-query-assistant">Honeycomb‚Äôs Natural Language Query Assistant</a>: Initially, the ‚Äúprogramming manual‚Äù was provided in the prompt together with n-shot examples for in-context learning. While this worked decently, fine-tuning the model led to better output on the syntax and rules of the domain-specific language. - <a href="https://www.youtube.com/watch?v=B_DMMlDuJB0">Rechat‚Äôs Lucy</a>: The LLM needed to generate responses in a very specific format that combined structured and unstructured data for the frontend to render correctly. Fine-tuning was essential to get it to work consistently.</p>
<p>Nonetheless, while fine-tuning can be effective, it comes with significant costs. We have to annotate fine-tuning data, finetune and evaluate models, and eventually self-host them. Thus, consider if the higher upfront cost is worth it. If prompting gets you 90% of the way there, then fine-tuning may not be worth the investment. However, if we do decide to finetune, to reduce the cost of collecting human annotated data, we can <a href="https://eugeneyan.com/writing/synthetic/">generate and finetune on synthetic data</a>, or <a href="https://eugeneyan.com/writing/finetuning/">bootstrap on open-source data</a>.</p>
</section>
</section>
<section id="evaluation-monitoring" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-monitoring">Evaluation &amp; Monitoring</h2>
<p>Evaluating LLMs can be a minefield. The inputs and the outputs of LLMs are arbitrary text, and the tasks we set them to are varied. Nonetheless, rigorous and thoughtful evals are critical‚Äîit‚Äôs no coincidence that technical leaders at OpenAI <a href="https://twitter.com/eugeneyan/status/1701692908074873036">work on evaluation and give feedback on individual evals</a>.</p>
<p>Evaluating LLM applications invites a diversity of definitions and reductions: it‚Äôs simply unit testing, or it‚Äôs more like observability, or maybe it‚Äôs just data science. We have found all of these perspectives useful. In the following section, we provide some lessons we‚Äôve learned about what is important in building evals and monitoring pipelines.</p>
<section id="create-a-few-assertion-based-unit-tests-from-real-inputoutput-samples" class="level3">
<h3 class="anchored" data-anchor-id="create-a-few-assertion-based-unit-tests-from-real-inputoutput-samples">Create a few assertion-based unit tests from real input/output samples</h3>
<p>Create <a href="https://hamel.dev/blog/posts/evals/#level-1-unit-tests">unit tests (i.e., assertions)</a> consisting of samples of inputs and outputs from production, with expectations for outputs based on at least three criteria. While three criteria might seem arbitrary, it‚Äôs a practical number to start with; fewer might indicate that your task isn‚Äôt sufficiently defined or is too open-ended, like a general-purpose chatbot. These unit tests, or assertions, should be triggered by any changes to the pipeline, whether it‚Äôs editing a prompt, adding new context via RAG, or other modifications. This <a href="https://hamel.dev/blog/posts/evals/#step-1-write-scoped-tests">write-up has an example</a> of an assertion-based test for an actual use case.</p>
<p>Consider beginning with assertions that specify phrases or ideas to either include or exclude in all responses. Also consider checks to ensure that word, item, or sentence counts lie within a range. For other kinds of generation, assertions can look different. <a href="https://www.semanticscholar.org/paper/Execution-Based-Evaluation-for-Open-Domain-Code-Wang-Zhou/1bed34f2c23b97fd18de359cf62cd92b3ba612c3">Execution-evaluation</a> is a powerful method for evaluating code-generation, wherein you run the generated code and determine that the state of runtime is sufficient for the user-request.</p>
<p>As an example, if the user asks for a new function named foo; then after executing the agent‚Äôs generated code, foo should be callable! One challenge in execution-evaluation is that the agent code frequently leaves the runtime in slightly different form than the target code. It can be effective to ‚Äúrelax‚Äù assertions to the absolute most weak assumptions that any viable answer would satisfy.</p>
<p>Finally, using your product as intended for customers (i.e., ‚Äúdogfooding‚Äù) can provide insight into failure modes on real-world data. This approach not only helps identify potential weaknesses, but also provides a useful source of production samples that can be converted into evals.</p>
</section>
<section id="llm-as-judge-can-work-somewhat-but-its-not-a-silver-bullet" class="level3">
<h3 class="anchored" data-anchor-id="llm-as-judge-can-work-somewhat-but-its-not-a-silver-bullet">LLM-as-Judge can work (somewhat), but it‚Äôs not a silver bullet</h3>
<p>LLM-as-Judge, where we use a strong LLM to evaluate the output of other LLMs, has been met with skepticism by some. (Some of us were initially huge skeptics.) Nonetheless, when implemented well, LLM-as-Judge achieves decent correlation with human judgements, and can at least help build priors about how a new prompt or technique may perform. Specifically, when doing pairwise comparisons (e.g., control vs.&nbsp;treatment), LLM-as-Judge typically gets the direction right though the magnitude of the win/loss may be noisy.</p>
<p>Here are some suggestions to get the most out of LLM-as-Judge: - Use pairwise comparisons: Instead of asking the LLM to score a single output on a <a href="https://en.wikipedia.org/wiki/Likert_scale">Likert</a> scale, present it with two options and ask it to select the better one. This tends to lead to more stable results. - Control for position bias: The order of options presented can bias the LLM‚Äôs decision. To mitigate this, do each pairwise comparison twice, swapping the order of pairs each time. Just be sure to attribute wins to the right option after swapping! - Allow for ties: In some cases, both options may be equally good. Thus, allow the LLM to declare a tie so it doesn‚Äôt have to arbitrarily pick a winner. - Use Chain-of-Thought: Asking the LLM to explain its decision before giving a final preference can increase eval reliability. As a bonus, this allows you to use a weaker but faster LLM and still achieve similar results. Because frequently this part of the pipeline is in batch mode, the extra latency from CoT isn‚Äôt a problem. - Control for response length: LLMs tend to bias toward longer responses. To mitigate this, ensure response pairs are similar in length.</p>
<p>One particularly powerful application of LLM-as-Judge is checking a new prompting strategy against regression. If you have tracked a collection of production results, sometimes you can rerun those production examples with a new prompting strategy, and use LLM-as-Judge to quickly assess where the new strategy may suffer.</p>
<p>Here‚Äôs an example of a <a href="https://hamel.dev/blog/posts/evals/#automated-evaluation-w-llms">simple but effective approach</a> to iterate on LLM-as-Judge, where we simply log the LLM response, judge‚Äôs critique (i.e., CoT), and final outcome. They are then reviewed with stakeholders to identify areas for improvement. Over three iterations, agreement with human and LLM improved from 68% to 94%!</p>
<p><img src="https://hamel.dev/blog/posts/evals/images/spreadsheet.png" class="img-fluid"></p>
<p>LLM-as-Judge is not a silver bullet though. There are subtle aspects of language where even the strongest models fail to evaluate reliably. In addition, we‚Äôve found that <a href="https://eugeneyan.com/writing/finetuning/">conventional classifiers</a> and reward models can achieve higher accuracy than LLM-as-Judge, and with lower cost and latency. For code generation, LLM-as-Judge can be weaker than more direct evaluation strategies like execution-evaluation.</p>
</section>
<section id="the-intern-test-for-evaluating-generations" class="level3">
<h3 class="anchored" data-anchor-id="the-intern-test-for-evaluating-generations">The ‚Äúintern test‚Äù for evaluating generations</h3>
<p>We like to use the following ‚Äúintern test‚Äù when evaluating generations: If you took the exact input to the language model, including the context, and gave it to an average college student in the relevant major as a task, could they succeed? How long would it take?</p>
<p>If the answer is no because the LLM lacks the required knowledge, consider ways to enrich the context.</p>
<p>If the answer is no and we simply can‚Äôt improve the context to fix it, then we may have hit a task that‚Äôs too hard for contemporary LLMs.</p>
<p>If the answer is yes, but it would take a while, we can try to reduce the complexity of the task. Is it decomposable? Are there aspects of the task that can be made more templatized?</p>
<p>If the answer is yes, they would get it quickly, then it‚Äôs time to dig into the data. What‚Äôs the model doing wrong? Can we find a pattern of failures? Try asking the model to explain itself before or after it responds, to help you build a theory of mind.</p>
</section>
<section id="overemphasizing-certain-evals-can-hurt-overall-performance" class="level3">
<h3 class="anchored" data-anchor-id="overemphasizing-certain-evals-can-hurt-overall-performance">Overemphasizing certain evals can hurt overall performance</h3>
<p>‚ÄúWhen a measure becomes a target, it ceases to be a good measure.‚Äù ‚Äî Goodhart‚Äôs Law.</p>
<p>An example of this is the Needle-in-a-Haystack (NIAH) eval. The original eval helped quantify model recall as context sizes grew, as well as how recall is affected by needle position. However, it‚Äôs been so overemphasized that it‚Äôs featured as <a href="https://arxiv.org/abs/2403.05530">Figure 1 for Gemini 1.5‚Äôs report</a>. The eval involves inserting a specific phrase (‚ÄúThe special magic {city} number is: {number}‚Äù) into a long document which repeats the essays of Paul Graham, and then prompting the model to recall the magic number.</p>
<p>While some models achieve near-perfect recall, it‚Äôs questionable whether NIAH truly reflects the reasoning and recall abilities needed in real-world applications. Consider a more practical scenario: Given the transcript of an hour-long meeting, can the LLM summarize the key decisions and next steps, as well as correctly attribute each item to the relevant person? This task is more realistic, going beyond rote memorization and also considering the ability to parse complex discussions, identify relevant information, and synthesize summaries.</p>
<p>Here‚Äôs an example of a <a href="https://observablehq.com/@shreyashankar/needle-in-the-real-world-experiments">practical NIAH eval</a>. Using <a href="https://github.com/wyim/aci-bench/tree/main/data/challenge_data">transcripts of doctor-patient video calls</a>, the LLM is queried about the patient‚Äôs medication. It also includes a more challenging NIAH, inserting a phrase for random ingredients for pizza toppings, such as ‚Äú<em>The secret ingredients needed to build the perfect pizza are: Espresso-soaked dates, Lemon and Goat cheese.</em>‚Äù. Recall was around 80% on the medication task and 30% on the pizza task.</p>
<p><img src="images/niah.png" class="img-fluid" style="width:80.0%"></p>
<p>Tangentially, an overemphasis on NIAH evals can lead to lower performance on extraction and summarization tasks. Because these LLMs are so finetuned to attend to every sentence, they may start to treat irrelevant details and distractors as important, thus including them in the final output (when they shouldn‚Äôt!)</p>
<p>This could also apply to other evals and use cases. For example, summarization. An emphasis on factual consistency could lead to summaries that are less specific (and thus less likely to be factually inconsistent) and possibly less relevant. Conversely, an emphasis on writing style and eloquence could lead to more flowery, marketing-type language that could introduce factual inconsistencies.</p>
</section>
<section id="simplify-annotation-to-binary-tasks-or-pairwise-comparisons" class="level3">
<h3 class="anchored" data-anchor-id="simplify-annotation-to-binary-tasks-or-pairwise-comparisons">Simplify annotation to binary tasks or pairwise comparisons</h3>
<p>Providing open-ended feedback or ratings for model output on a <a href="https://en.wikipedia.org/wiki/Likert_scale">Likert scale</a> is cognitively demanding. As a result, the data collected is more noisy‚Äîdue to variability among human raters‚Äîand thus less useful. A more effective approach is to simplify the task and reduce the cognitive burden on annotators. Two tasks that work well are binary classifications and pairwise comparisons.</p>
<p>In binary classifications, annotators are asked to make a simple yes-or-no judgment on the model‚Äôs output. They might be asked whether the generated summary is factually consistent with the source document, or whether the proposed response is relevant, or if it contains toxicity. Compared to the Likert scale, binary decisions are more precise, have higher consistency among raters, and lead to higher throughput. This was how <a href="https://doordash.engineering/2020/08/28/overcome-the-cold-start-problem-in-menu-item-tagging/">Doordash setup their labeling queues</a> for tagging menu items though a tree of yes-no questions.</p>
<p>In pairwise comparisons, the annotator is presented with a pair of model responses and asked which is better. Because it‚Äôs easier for humans to say ‚ÄúA is better than B‚Äù than to assign an individual score to either A or B individually, this leads to faster and more reliable annotations (over Likert scales). At a <a href="https://www.youtube.com/watch?v=CzR3OrOkM9w">Llama2 meetup</a>, Thomas Scialom, an author on the Llama2 paper, confirmed that pairwise-comparisons were faster and cheaper than collecting supervised finetuning data such as written responses. The former‚Äôs cost is $3.5 per unit while the latter‚Äôs cost is $25 per unit.</p>
<p>If you‚Äôre starting to write labeling guidelines, here are some <a href="https://eugeneyan.com/writing/labeling-guidelines/">reference guidelines</a> from Google and Bing Search.</p>
</section>
<section id="reference-free-evals-and-guardrails-can-be-used-interchangeably" class="level3">
<h3 class="anchored" data-anchor-id="reference-free-evals-and-guardrails-can-be-used-interchangeably">(Reference-free) evals and guardrails can be used interchangeably</h3>
<p>Guardrails help to catch inappropriate or harmful content while evals help to measure the quality and accuracy of the model‚Äôs output. In the case of reference-free evals, they may be considered two sides of the same coin. Reference-free evals are evaluations that don‚Äôt rely on a ‚Äúgolden‚Äù reference, such as a human-written answer, and can assess the quality of output based solely on the input prompt and the model‚Äôs response.</p>
<p>Some examples of these are <a href="https://eugeneyan.com/writing/evals/#summarization-consistency-relevance-length">summarization evals</a>, where we only have to consider the input document to evaluate the summary on factual consistency and relevance. If the summary scores poorly on these metrics, we can choose not to display it to the user, effectively using the eval as a guardrail. Similarly, reference-free <a href="https://eugeneyan.com/writing/evals/#translation-statistical--learned-evals-for-quality">translation evals</a> can assess the quality of a translation without needing a human-translated reference, again allowing us to use it as a guardrail.</p>
</section>
<section id="llms-will-return-output-even-when-they-shouldnt" class="level3">
<h3 class="anchored" data-anchor-id="llms-will-return-output-even-when-they-shouldnt">LLMs will return output even when they shouldn‚Äôt</h3>
<p>A key challenge when working with LLMs is that they‚Äôll often generate output even when they shouldn‚Äôt. This can lead to harmless but nonsensical responses, or more egregious defects like toxicity or dangerous content. For example, when asked to extract specific attributes or metadata from a document, an LLM may confidently return values even when those values don‚Äôt actually exist. Alternatively, the model may respond in a language other than English because we provided non-English documents in the context.</p>
<p>While we can try to prompt the LLM to return a ‚Äúnot applicable‚Äù or ‚Äúunknown‚Äù response, it‚Äôs not foolproof. Even when the log probabilities are available, they‚Äôre a poor indicator of output quality. While log probs indicate the likelihood of a token appearing in the output, they don‚Äôt necessarily reflect the correctness of the generated text. On the contrary, for instruction-tuned models that are trained to respond to queries and generate coherent response, log probabilities may not be well-calibrated. Thus, while a high log probability may indicate that the output is fluent and coherent, it doesn‚Äôt mean it‚Äôs accurate or relevant.</p>
<p>While careful prompt engineering can help to some extent, we should complement it with robust guardrails that detect and filter/regenerate undesired output. For example, OpenAI provides a <a href="https://platform.openai.com/docs/guides/moderation">content moderation API</a> that can identify unsafe responses such as hate speech, self-harm, or sexual output. Similarly, there are numerous packages for <a href="https://github.com/topics/pii-detection">detecting personally identifiable information</a> (PII). One benefit is that guardrails are largely agnostic of the use case and can thus be applied broadly to all output in a given language. In addition, with precise retrieval, our system can deterministically respond ‚ÄúI don‚Äôt know‚Äù if there are no relevant documents.</p>
<p>A corollary here is that LLMs may fail to produce outputs when they are expected to. This can happen for various reasons, from straightforward issues like long tail latencies from API providers to more complex ones such as outputs being blocked by content moderation filters. As such, it‚Äôs important to consistently log inputs and (potentially a lack of) outputs for debugging and monitoring.</p>
</section>
<section id="hallucinations-are-a-stubborn-problem" class="level3">
<h3 class="anchored" data-anchor-id="hallucinations-are-a-stubborn-problem">Hallucinations are a stubborn problem</h3>
<p>Unlike content safety or PII defects which have a lot of attention and thus seldom occur, factual inconsistencies are stubbornly persistent and more challenging to detect. They‚Äôre more common and occur at a baseline rate of 5 - 10%, and from what we‚Äôve learned from LLM providers, it can be challenging to get it below 2%, even on simple tasks such as summarization.</p>
<p>To address this, we can combine prompt engineering (upstream of generation) and factual inconsistency guardrails (downstream of generation). For prompt engineering, techniques like CoT help reduce hallucination by getting the LLM to explain its reasoning before finally returning the output. Then, we can apply a <a href="https://eugeneyan.com/writing/finetuning/">factual inconsistency guardrail</a> to assess the factuality of summaries and filter or regenerate hallucinations. In some cases, hallucinations can be deterministically detected. When using resources from RAG retrieval, if the output is structured and identifies what the resources are, you should be able to manually verify they‚Äôre sourced from the input context.</p>
</section>
</section>
</section>
<section id="operational-day-to-day-and-org-concerns" class="level1">
<h1>Operational: Day-to-day and org concerns</h1>
<section id="data" class="level2">
<h2 class="anchored" data-anchor-id="data">Data</h2>
<p>Just as the quality of ingredients determines the dish‚Äôs taste, the quality of input data constrains the performance of machine learning systems. In addition, output data is the only way to tell whether the product is working or not. All the authors focus tightly on the data, looking at inputs and outputs for several hours a week to better understand the data distribution: its modes, its edge cases, and the limitations of models of it.</p>
<section id="check-for-development-prod-skew" class="level3">
<h3 class="anchored" data-anchor-id="check-for-development-prod-skew">Check for development-prod skew</h3>
<p>A common source of errors in traditional machine learning pipelines is <em>train-serve skew</em>. This happens when the data used in training differs from what the model encounters in production. Although we can use LLMs without training or fine-tuning, hence there‚Äôs no training set, a similar issue arises with development-prod data skew. Essentially, the data we test our systems on during development should mirror what the systems will face in production. If not, we might find our production accuracy suffering.</p>
<p>LLM development-prod skew can be categorized into two types: structural and content-based. Structural skew includes issues like formatting discrepancies, such as differences between a JSON dictionary with a list-type value and a JSON list, inconsistent casing, and errors like typos or sentence fragments. These errors can lead to unpredictable model performance because different LLMs are trained on specific data formats, and prompts can be highly sensitive to minor changes. Content-based or ‚Äúsemantic‚Äù skew refers to differences in the meaning or context of the data.&nbsp;</p>
<p>As in traditional ML, it‚Äôs useful to periodically measure skew between the LLM input/output pairs. Simple metrics like the length of inputs and outputs or specific formatting requirements (e.g., JSON or XML) are straightforward ways to track changes. For more ‚Äúadvanced‚Äù drift detection, consider clustering embeddings of input/output pairs to detect semantic drift, such as shifts in the topics users are discussing, which could indicate they are exploring areas the model hasn‚Äôt been exposed to before.&nbsp;</p>
<p>When testing changes, such as prompt engineering, ensure that hold-out datasets are current and reflect the most recent types of user interactions. For example, if typos are common in production inputs, they should also be present in the hold-out data. Beyond just numerical skew measurements, it‚Äôs beneficial to perform qualitative assessments on outputs. Regularly reviewing your model‚Äôs outputs‚Äîa practice colloquially known as ‚Äúvibe checks‚Äù‚Äîensures that the results align with expectations and remain relevant to user needs. Finally, incorporating nondeterminism into skew checks is also useful‚Äîby running the pipeline multiple times for each input in our testing dataset and analyzing all outputs, we increase the likelihood of catching anomalies that might occur only occasionally.</p>
</section>
<section id="look-at-samples-of-llm-inputs-and-outputs-every-day" class="level3">
<h3 class="anchored" data-anchor-id="look-at-samples-of-llm-inputs-and-outputs-every-day">Look at samples of LLM inputs and outputs every day</h3>
<p>LLMs are dynamic and constantly evolving. Despite their impressive zero-shot capabilities and often delightful outputs, their failure modes can be highly unpredictable. For custom tasks, regularly reviewing data samples is essential to developing an intuitive understanding of how LLMs perform.</p>
<p>Input-output pairs from production are the ‚Äúreal things, real places‚Äù (<em>genchi genbutsu</em>) of LLM applications, and they cannot be substituted. <a href="https://arxiv.org/abs/2404.12272">Recent research</a> highlighted that developers‚Äô perceptions of what constitutes ‚Äúgood‚Äù and ‚Äúbad‚Äù outputs shift as they interact with more data (i.e., <em>criteria drift</em>). While developers can come up with some criteria upfront for evaluating LLM outputs, these predefined criteria are often incomplete. For instance, during the course of development, we might update the prompt to increase the probability of good responses and decrease the probability of bad ones. This iterative process of evaluation, reevaluation, and criteria update is necessary, as it‚Äôs difficult to predict either LLM behavior or human preference without directly observing the outputs.</p>
<p>To manage this effectively, we should log LLM inputs and outputs. By examining a sample of these logs daily, we can quickly identify and adapt to new patterns or failure modes. When we spot a new issue, we can immediately write an assertion or eval around it. Similarly, any updates to failure mode definitions should be reflected in the evaluation criteria. These ‚Äúvibe checks‚Äù are signals of bad outputs; code and assertions operationalize them. Finally, this attitude must be socialized, for example by adding review or annotation of inputs and outputs to your on-call rotation.</p>
</section>
</section>
<section id="working-with-models" class="level2">
<h2 class="anchored" data-anchor-id="working-with-models">Working with models</h2>
<p>With LLM APIs, we can rely on intelligence from a handful of providers. While this is a boon, these dependencies also involve trade-offs on performance, latency, throughput, and cost. Also, as newer, better models drop (almost every month in the past year), we should be prepared to update our products as we deprecate old models and migrate to newer models. In this section, we share our lessons from working with technologies we don‚Äôt have full control over, where the models can‚Äôt be self-hosted and managed.</p>
<section id="generate-structured-output-to-ease-downstream-integration" class="level3">
<h3 class="anchored" data-anchor-id="generate-structured-output-to-ease-downstream-integration">Generate structured output to ease downstream integration</h3>
<p>For most real-world use cases, the output of an LLM will be consumed by a downstream application via some machine-readable format. For example, <a href="https://www.youtube.com/watch?v=B_DMMlDuJB0">ReChat</a>, a real-estate CRM, required structured responses for the front end to render widgets. Similarly, <a href="https://martinfowler.com/articles/building-boba.html">Boba</a>, a tool for generating product strategy ideas, needed structured output with fields for title, summary, plausibility score, and time horizon. Finally, LinkedIn shared about <a href="https://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product">constraining the LLM to generate YAML</a>, which is then used to decide which skill to use, as well as provide the parameters to invoke the skill.</p>
<p>This application pattern is an extreme version of Postel‚Äôs Law: be liberal in what you accept (arbitrary natural language) and conservative in what you send (typed, machine-readable objects). As such, we expect it to be extremely durable.</p>
<p>Currently, <a href="https://github.com/jxnl/instructor">Instructor</a> and <a href="https://github.com/outlines-dev/outlines">Outlines</a> are the de facto standards for coaxing structured output from LLMs. If you‚Äôre using an LLM API (e.g., Anthropic, OpenAI), use Instructor; if you‚Äôre working with a self-hosted model (e.g., Huggingface), use Outlines.</p>
</section>
<section id="migrating-prompts-across-models-is-a-pain-in-the-ass" class="level3">
<h3 class="anchored" data-anchor-id="migrating-prompts-across-models-is-a-pain-in-the-ass">Migrating prompts across models is a pain in the ass</h3>
<p>Sometimes, our carefully crafted prompts work superbly with one model but fall flat with another. This can happen when we‚Äôre switching between various model providers, as well as when we upgrade across versions of the same model.&nbsp;</p>
<p>For example, Voiceflow found that <a href="https://www.voiceflow.com/blog/how-much-do-chatgpt-versions-affect-real-world-performance">migrating from gpt-3.5-turbo-0301 to gpt-3.5-turbo-1106 led to a 10% drop</a> on their intent classification task. (Thankfully, they had evals!) Similarly, <a href="https://www.godaddy.com/resources/news/llm-from-the-trenches-10-lessons-learned-operationalizing-models-at-godaddy#h-3-prompts-aren-t-portable-across-models">GoDaddy observed a trend in the positive direction</a>, where upgrading to version 1106 narrowed the performance gap between gpt-3.5-turbo and gpt-4. (Or, if you‚Äôre a glass-half-full person, you might be disappointed that gpt-4‚Äôs lead was reduced with the new upgrade)</p>
<p>Thus, if we have to migrate prompts across models, expect it to take more time than simply swapping the API endpoint. Don‚Äôt assume that plugging in the same prompt will lead to similar or better results. Also, having reliable, automated evals helps with measuring task performance before and after migration, and reduces the effort needed for manual verification.</p>
</section>
<section id="version-and-pin-your-models" class="level3">
<h3 class="anchored" data-anchor-id="version-and-pin-your-models">Version and pin your models</h3>
<p>In any machine learning pipeline, ‚Äú<a href="https://papers.nips.cc/paper_files/paper/2015/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html">changing anything changes everything</a>‚Äù. This is particularly relevant as we rely on components like large language models (LLMs) that we don‚Äôt train ourselves and that can change without our knowledge.</p>
<p>Fortunately, many model providers offer the option to ‚Äúpin‚Äù specific model versions (e.g., gpt-4-turbo-1106). This enables us to use a specific version of the model weights, ensuring they remain unchanged. Pinning model versions in production can help avoid unexpected changes in model behavior, which could lead to customer complaints about issues that may crop up when a model is swapped, such as overly verbose outputs or other unforeseen failure modes.</p>
<p>Additionally, consider maintaining a shadow pipeline that mirrors your production setup but uses the latest model versions. This enables safe experimentation and testing with new releases. Once you‚Äôve validated the stability and quality of the outputs from these newer models, you can confidently update the model versions in your production environment.</p>
</section>
<section id="choose-the-smallest-model-that-gets-the-job-done" class="level3">
<h3 class="anchored" data-anchor-id="choose-the-smallest-model-that-gets-the-job-done">Choose the smallest model that gets the job done</h3>
<p>When working on a new application, it‚Äôs tempting to use the biggest, most powerful model available. But once we‚Äôve established that the task is technically feasible, it‚Äôs worth experimenting if a smaller model can achieve comparable results.</p>
<p>The benefits of a smaller model are lower latency and cost. While it may be weaker, techniques like chain-of-thought, n-shot prompts, and in-context learning can help smaller models punch above their weight. Beyond LLM APIs, fine-tuning our specific tasks can also help increase performance.</p>
<p>Taken together, a carefully crafted workflow using a smaller model can often match, or even surpass, the output quality of a single large model, while being faster and cheaper. For example, this <a href="https://twitter.com/mattshumer_/status/1770823530394833242">tweet</a> shares anecdata of how Haiku + 10-shot prompt outperforms zero-shot Opus and GPT-4. In the long term, we expect to see more examples of <a href="https://twitter.com/karpathy/status/1748043513156272416">flow-engineering</a> with smaller models as the optimal balance of output quality, latency, and cost.</p>
<p>As another example, take the humble classification task. Lightweight models like DistilBERT (67M parameters) are a surprisingly strong baseline. The 400M parameter DistilBART is another great option‚Äîwhen finetuned on open-source data, it could <a href="https://eugeneyan.com/writing/finetuning/">identify hallucinations with an ROC-AUC of 0.84</a>, surpassing most LLMs at less than 5% of latency and cost.</p>
<p>The point is, don‚Äôt overlook smaller models. While it‚Äôs easy to throw a massive model at every problem, with some creativity and experimentation, we can often find a more efficient solution.&nbsp;</p>
</section>
</section>
<section id="product" class="level2">
<h2 class="anchored" data-anchor-id="product">Product</h2>
<p>While new technology offers new possibilities, the principles of building great products are timeless. Thus, even if we‚Äôre solving new problems for the first time, we don‚Äôt have to reinvent the wheel on product design. There‚Äôs a lot to gain from grounding our LLM application development in solid product fundamentals, allowing us to deliver real value to the people we serve.</p>
<section id="involve-design-early-and-often" class="level3">
<h3 class="anchored" data-anchor-id="involve-design-early-and-often">Involve design early and often</h3>
<p>Having a designer will push you to understand and think deeply about how your product can be built and presented to users. We sometimes stereotype designers as folks who take things and make them pretty. But beyond just the user interface, they also rethink how the user experience can be improved, even if it means breaking existing rules and paradigms.</p>
<p>Designers are especially gifted at reframing the user‚Äôs needs into various forms. Some of these forms are more tractable to solve than others, and thus, they may offer more or fewer opportunities for AI solutions. Like many other products, building AI products should be centered around the job to be done, not the technology that powers them.</p>
<p>Focus on asking yourself: ‚ÄúWhat job is the user asking this product to do for them? Is that job something a chatbot would be good at? How about autocomplete? Maybe something different!‚Äù Consider the existing <a href="https://www.tidepool.so/blog/emerging-ux-patterns-for-generative-ai-apps-copilots">design patterns</a> and how they relate to the job-to-be-done. These are the invaluable assets that designers add to your team‚Äôs capabilities.</p>
</section>
<section id="design-your-ux-for-human-in-the-loop" class="level3">
<h3 class="anchored" data-anchor-id="design-your-ux-for-human-in-the-loop">Design your UX for Human-In-The-Loop</h3>
<p>One way to get quality annotations is to integrate Human-in-the-Loop (HITL) into the user experience (UX). By allowing users to provide feedback and corrections easily, we can improve the immediate output and collect valuable data to improve our models.</p>
<p>Imagine an e-commerce platform where users upload and categorize their products. There are several ways we could design the UX:</p>
<ul>
<li>The user manually selects the right product category; an LLM periodically checks new products and corrects miscategorization on the backend.</li>
<li>The user doesn‚Äôt select any category at all; an LLM periodically categorizes products on the backend (with potential errors).</li>
<li>An LLM suggests a product category in real-time, which the user can validate and update as needed.</li>
</ul>
<p>While all three approaches involve an LLM, they provide very different UXes. The first approach puts the initial burden on the user and has the LLM acting as a post-processing check. The second requires zero effort from the user but provides no transparency or control. The third strikes the right balance. By having the LLM suggest categories upfront, we reduce cognitive load on the user and they don‚Äôt have to learn our taxonomy to categorize their product! At the same time, by allowing the user to review and edit the suggestion, they have the final say in how their product is classified, putting control firmly in their hands. As a bonus, the third approach creates a <a href="https://eugeneyan.com/writing/llm-patterns/#collect-user-feedback-to-build-our-data-flywheel">natural feedback loop for model improvement</a>. Suggestions that are good are accepted (positive labels) and those that are bad are updated (negative followed by positive labels).</p>
<p>This pattern of suggestion, user validation, and data collection is commonly seen in several applications:</p>
<ul>
<li>Coding assistants: Where users can accept a suggestion (strong positive), accept and tweak a suggestion (positive), or ignore a suggestion (negative)</li>
<li>Midjourney: Where users can choose to upscale and download the image (strong positive), vary an image (positive), or generate a new set of images (negative)</li>
<li>Chatbots: Where users can provide thumbs up (positive) or thumbs down (negative) on responses, or choose to regenerate a response if it was really bad (strong negative).</li>
</ul>
<p>Feedback can be explicit or implicit. Explicit feedback is information users provide in response to a request by our product; implicit feedback is information we learn from user interactions without needing users to deliberately provide feedback. Coding assistants and Midjourney are examples of implicit feedback while thumbs up and thumb downs are explicit feedback. If we design our UX well, like coding assistants and Midjourney, we can collect plenty of implicit feedback to improve our product and models.</p>
</section>
<section id="prioritize-your-hierarchy-of-needs-ruthlessly" class="level3">
<h3 class="anchored" data-anchor-id="prioritize-your-hierarchy-of-needs-ruthlessly">Prioritize your hierarchy of needs ruthlessly</h3>
<p>As we think about putting our demo into production, we‚Äôll have to think about the requirements for:</p>
<ul>
<li>Reliability: 99.9% uptime, adherence to structured output</li>
<li>Harmlessness: Not generate offensive, NSFW, or otherwise harmful content</li>
<li>Factual consistency: Being faithful to the context provided, not making things up</li>
<li>Usefulness: Relevant to the users‚Äô needs and request</li>
<li>Scalability: Latency SLAs, supported throughput</li>
<li>Cost: Because we don‚Äôt have unlimited budget</li>
<li>And more: Security, privacy, fairness, GDPR, DMA, etc, etc.</li>
</ul>
<p>If we try to tackle all these requirements at once, we‚Äôre never going to ship anything. Thus, we need to prioritize. Ruthlessly. This means being clear what is non-negotiable (e.g., reliability, harmlessness) without which our product can‚Äôt function or won‚Äôt be viable. It‚Äôs all about identifying the minimum lovable product. We have to accept that the first version won‚Äôt be perfect, and just launch and iterate.</p>
</section>
<section id="calibrate-your-risk-tolerance-based-on-the-use-case" class="level3">
<h3 class="anchored" data-anchor-id="calibrate-your-risk-tolerance-based-on-the-use-case">Calibrate your risk tolerance based on the use case</h3>
<p>When deciding on the language model and level of scrutiny of an application, consider the use case and audience. For a customer-facing chatbot offering medical or financial advice, we‚Äôll need a very high bar for safety and accuracy. Mistakes or bad output could cause real harm and erode trust. But for less critical applications, such as a recommender system, or internal-facing applications like content classification or summarization, excessively strict requirements only slow progress without adding much value.</p>
<p>This aligns with a recent <a href="https://a16z.com/generative-ai-enterprise-2024/">a16z report</a> showing that many companies are moving faster with internal LLM applications compared to external ones. By experimenting with AI for internal productivity, organizations can start capturing value while learning how to manage risk in a more controlled environment. Then, as they gain confidence, they can expand to customer-facing use cases.</p>
</section>
</section>
<section id="team-roles" class="level2">
<h2 class="anchored" data-anchor-id="team-roles">Team &amp; Roles</h2>
<p>No job function is easy to define, but writing a job description for the work in this new space is more challenging than others. We‚Äôll forgo venn diagrams of intersecting job titles, or suggestions for job descriptions. We will, however, submit to the existence of a new role‚Äîthe AI engineer‚Äîand discuss its place. Importantly, we‚Äôll discuss the rest of the team and how responsibilities should be assigned.</p>
<section id="focus-on-process-not-tools" class="level3">
<h3 class="anchored" data-anchor-id="focus-on-process-not-tools">Focus on process, not tools</h3>
<p>When faced with new paradigms, such as LLMs, software engineers tend to favor tools. As a result, we overlook the problem and process the tool was supposed to solve. In doing so, many engineers assume accidental complexity, which has negative consequences for the team‚Äôs long-term productivity.</p>
<p>For example, <a href="https://hamel.dev/blog/posts/prompt/">this write-up</a> discusses how certain tools can automatically create prompts for large language models. It argues (rightfully IMHO) that engineers who use these tools without first understanding the problem-solving methodology or process end up taking on unnecessary technical debt.</p>
<p>In addition to accidental complexity, tools are often underspecified. For example, there is a growing industry of LLM evaluation tools that offer ‚ÄúLLM Evaluation In A Box‚Äù with generic evaluators for toxicity, conciseness, tone, etc. We have seen many teams adopt these tools without thinking critically about the specific failure modes of their domains. Contrast this to EvalGen. It focuses on teaching users the process of creating domain-specific evals by deeply involving the user each step of the way, from specifying criteria, to labeling data, to checking evals. The software leads the user through a workflow that looks like this:</p>
<p><img src="images/evalgen.png" class="img-fluid"></p>
<p><a href="https://arxiv.org/abs/2404.12272">Shankar, S., et al.&nbsp;(2024). Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences. Retrieved from https://arxiv.org/abs/2404.12272</a></p>
<p>EvalGen guides the user through a best practice of crafting LLM evaluations, namely:</p>
<ol type="1">
<li>Defining domain-specific tests (bootstrapped automatically from the prompt). These are defined as either assertions with code or with LLM-as-a-Judge.</li>
<li>The importance of aligning the tests with human judgment, so that the user can check that the tests capture the specified criteria.</li>
<li>Iterating on your tests as the system (prompts, etc) changes.&nbsp;</li>
</ol>
<p>EvalGen provides developers with a mental model of the evaluation building process without anchoring them to a specific tool. We have found that after providing AI Engineers with this context, they often decide to select leaner tools or build their own.&nbsp;&nbsp;</p>
<p>There are too many components of LLMs beyond prompt writing and evaluations to list exhaustively here.&nbsp; However, it is important that AI Engineers seek to understand the processes before adopting tools.</p>
</section>
<section id="always-be-experimenting" class="level3">
<h3 class="anchored" data-anchor-id="always-be-experimenting">Always be experimenting</h3>
<p>ML products are deeply intertwined with experimentation. Not only the A/B, Randomized Control Trials kind, but the frequent attempts at modifying the smallest possible components of your system, and doing offline evaluation. The reason why everyone is so hot for evals is not actually about trustworthiness and confidence‚Äîit‚Äôs about enabling experiments! The better your evals, the faster you can iterate on experiments, and thus the faster you can converge on the best version of your system.&nbsp;</p>
<p>It‚Äôs common to try different approaches to solving the same problem because experimentation is so cheap now. The high-cost of collecting data and training a model is minimized‚Äîprompt engineering costs little more than human time. Position your team so that everyone is taught the basics of prompt engineering. This encourages everyone to experiment and leads to diverse ideas from across the organization.</p>
<p>Additionally, don‚Äôt only experiment to explore‚Äîalso use them to exploit! Have a working version of a new task? Consider having someone else on the team approach it differently. Try doing it another way that‚Äôll be faster. Investigate prompt techniques like Chain-of-Thought or Few-Shot to make it higher quality. Don‚Äôt let your tooling hold you back on experimentation; if it is, rebuild it, or buy something to make it better.&nbsp;</p>
<p>Finally, during product/project planning, set aside time for building evals and running multiple experiments. Think of the product spec for engineering products, but add to it clear criteria for evals. And during roadmapping, don‚Äôt underestimate the time required for experimentation‚Äîexpect to do multiple iterations of development and evals before getting the green light for production.</p>
</section>
<section id="empower-everyone-to-use-new-ai-technology" class="level3">
<h3 class="anchored" data-anchor-id="empower-everyone-to-use-new-ai-technology">Empower everyone to use new AI technology</h3>
<p>As generative AI increases in adoption, we want the entire team‚Äînot just the experts‚Äîto understand and feel empowered to use this new technology. There‚Äôs no better way to develop intuition for how LLMs work (e.g., latencies, failure modes, UX) than to, well, use them. LLMs are relatively accessible: You don‚Äôt need to know how to code to improve performance for a pipeline, and everyone can start contributing via prompt engineering and evals.</p>
<p>A big part of this is education. It can start as simple as the basics of prompt engineering, where techniques like n-shot prompting and CoT help condition the model towards the desired output. Folks who have the knowledge can also educate about the more technical aspects, such as how LLMs are autoregressive in nature. In other words, while input tokens are processed in parallel, output tokens are generated sequentially. As a result, latency is more a function of output length than input length‚Äîthis is a key consideration when designing UXes and setting performance expectations.</p>
<p>We can also go further and provide opportunities for hands-on experimentation and exploration. A hackathon perhaps? While it may seem expensive to have an entire team spend a few days hacking on speculative projects, the outcomes may surprise you. We know of a team that, through a hackathon, accelerated and almost completed their three-year roadmap within a year. Another team had a hackathon that led to paradigm shifting UXes that are now possible thanks to LLMs, which are now prioritized for the year and beyond.</p>
</section>
<section id="dont-fall-into-the-trap-of-ai-engineering-is-all-i-need" class="level3">
<h3 class="anchored" data-anchor-id="dont-fall-into-the-trap-of-ai-engineering-is-all-i-need">Don‚Äôt fall into the trap of ‚ÄúAI Engineering is all I need‚Äù</h3>
<p>As new job titles are coined, there is an initial tendency to overstate the capabilities associated with these roles. This often results in a painful correction as the actual scope of these jobs becomes clear. Newcomers to the field, as well as hiring managers, might make exaggerated claims or have inflated expectations. Notable examples over the last decade include:</p>
<ul>
<li>Data Scientist: ‚Äú<a href="https://x.com/josh_wills/status/198093512149958656">someone who is better at statistics than any software engineer and better at software engineering than any statistician</a>.‚Äù&nbsp;&nbsp;</li>
<li>Machine Learning Engineer (MLE): a software engineering-centric view of machine learning&nbsp;</li>
</ul>
<p>Initially, many assumed that data scientists alone were sufficient for data-driven projects. However, it became apparent that data scientists must collaborate with software and data engineers to develop and deploy data products effectively.&nbsp;</p>
<p>This misunderstanding has shown up again with the new role of AI Engineer, with some teams believing that AI Engineers are all you need. In reality, building machine learning or AI products requires a <a href="https://papers.nips.cc/paper_files/paper/2015/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html">broad array of specialized roles</a>. We‚Äôve consulted with more than a dozen companies on AI products and have consistently observed that they fall into the trap of believing that ‚ÄúAI Engineering is all you need.‚Äù As a result, products often struggle to scale beyond a demo as companies overlook crucial aspects involved in building a product.</p>
<p>For example, evaluation and measurement are crucial for scaling a product beyond vibe checks. The skills for effective evaluation align with some of the strengths traditionally seen in machine learning engineers‚Äîa team composed solely of AI Engineers will likely lack these skills. Co-author Hamel Husain illustrates the importance of these skills in his recent work around detecting <a href="https://github.com/hamelsmu/ft-drift">data drift</a> and <a href="https://hamel.dev/blog/posts/evals/">designing domain-specific evals</a>.</p>
<p>Here is a rough progression of the types of roles you need, and when you‚Äôll need them, throughout the journey of building an AI product:</p>
<ol type="1">
<li>First, focus on building a product. This might include an AI engineer, but it doesn‚Äôt have to. AI Engineers are valuable for prototyping and iterating quickly on the product (UX, plumbing, etc).&nbsp;</li>
<li>Next, create the right foundations by instrumenting your system and collecting data. Depending on the type and scale of data, you might need platform and/or data engineers. You must also have systems for querying and analyzing this data to debug issues.</li>
<li>Next, you will eventually want to optimize your AI system. This doesn‚Äôt necessarily involve training models. The basics include steps like designing metrics, building evaluation systems, running experiments, optimizing RAG retrieval, debugging stochastic systems, and more. MLEs are really good at this (though AI engineers can pick them up too). It usually doesn‚Äôt make sense to hire an MLE unless you have completed the prerequisite steps.</li>
</ol>
<p>Aside from this, you need a domain expert at all times. At small companies, this would ideally be the founding team‚Äîand at bigger companies, product managers can play this role. Being aware of the progression and timing of roles is critical. Hiring folks at the wrong time (e.g., <a href="https://jxnl.co/writing/2024/04/08/hiring-mle-at-early-stage-companies/">hiring an MLE too early</a>) or building in the wrong order is a waste of time and money, and causes churn.&nbsp; Furthermore, regularly checking in with an MLE (but not hiring them full-time) during phases 1-2 will help the company build the right foundations.&nbsp;</p>
</section>
</section>
</section>
<section id="strategic-long-term-business-strategy-pending" class="level1">
<h1>Strategic: Long-term business strategy (pending)</h1>
<p>PENDING RELEASE (tentatively 6th June)</p>
<hr>
<section id="stay-in-touch" class="level3">
<h3 class="anchored" data-anchor-id="stay-in-touch">Stay In Touch</h3>
<p>If you found this useful and want updates on write-ups, courses, and activities, subscribe below.</p>
<script async="" data-uid="b3e2fda9e7" src="https://appliedllms.ck.page/b3e2fda9e7/index.js"></script>
<p>You can also find our individual contact information on our <a href="./about.html">about page</a>.</p>
</section>
<section id="acknowledgements" class="level3">
<h3 class="anchored" data-anchor-id="acknowledgements">Acknowledgements</h3>
<p>This series started as a conversation in a group chat, where Bryan quipped that he was inspired to write ‚ÄúA Year of AI Engineering‚Äù. Then, ‚ú®magic‚ú® happened, and we were all inspired to chip in and share what we‚Äôve learned so far.</p>
<p>The authors would like to thank Eugene for leading the bulk of the document integration and overall structure in addition to a large proportion of the lessons. Additionally, for primary editing responsibilities and document direction. The authors would like to thank Bryan for the spark that led to this writeup, restructuring the write-up into tactical, operational, and strategic sections and their intros, and for pushing us to think bigger on how we could reach and help the community. The authors would like to thank Charles for his deep dives on cost and LLMOps, as well as weaving the lessons to make them more coherent and tighter‚Äîyou have him to thank for this being 30 instead of 40 pages! The authors thank Hamel and Jason for their insights from advising clients and being on the front lines, for their broad generalizable learnings from clients, and for deep knowledge of tools. And finally, thank you Shreya for reminding us of the importance of evals and rigorous production practices and for bringing her research and original results.</p>
<p>Finally, we would like to thank all the teams who so generously shared your challenges and lessons in your own write-ups which we‚Äôve referenced throughout this series, along with the AI communities for your vibrant participation and engagement with this group.</p>
</section>
<section id="about-the-authors" class="level3">
<h3 class="anchored" data-anchor-id="about-the-authors">About the authors</h3>
<p>See the <a href="./about.html">about page</a> for more information on the authors.</p>
<p>If you found this useful, please cite this write-up as:</p>
<blockquote class="blockquote">
<p>Yan, Eugene, Bryan Bischof, Charles Frye, Hamel Husain, Jason Liu, and Shreya Shankar. 2024. ‚ÄòApplied LLMs - What We‚Äôve Learned From A Year of Building with LLMs‚Äô. Applied LLMs. 8 June 2024. https://applied-llms.org/.</p>
</blockquote>
<p>or</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode default code-with-copy"><code class="sourceCode default"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>@article{AppliedLLMs2024,</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  title = {What We've Learned From A Year of Building with LLMs},</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  author = {Yan, Eugene and Bischof, Bryan and Frye, Charles and Husain, Hamel and Liu, Jason and Shankar, Shreya},</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  journal = {Applied LLMs},</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  year = {2024},</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  month = {Jun},</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  url = {https://applied-llms.org/}</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/applied-llms\.org");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>